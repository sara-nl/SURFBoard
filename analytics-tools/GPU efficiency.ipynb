{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU efficiency\n",
    "\n",
    "## GPU compute efficiency methodology\n",
    "\n",
    "Separate the kernels into 2 groups:\n",
    "- kernels with a null FLOPs value\n",
    "- kernels with a non-null FLOPs value\n",
    "\n",
    "The GPU compute efficiency is calculated as the duration of the kernels with a non-null FLOPS value divided by the total duration.\n",
    "\n",
    "## GPU FLOPS efficiency methodology\n",
    "\n",
    "GPU: NVIDIA Tesla K40m  \n",
    "FP32 theoretical flops: 4.29 TFLOPS\n",
    "\n",
    "Compute the total FLOPs of all the kernels.  \n",
    "Compute the total duration of all the kernels.  \n",
    "Compute the FLOPs per seconds of the GPU: `total_FLOPs / total_duration`  \n",
    "Compute the FLOPS efficiency: `FLOPs_per_sec_GPU / theoretical_flops`  \n",
    "\n",
    "This methodology is equivalent to weighting each kernel by its duration and then summing the weighted average of the FLOPs per seconds of each kernel.\n",
    "\n",
    "## GPU bandwidth efficiency methodology\n",
    "\n",
    "GPU: NVIDIA Tesla K40m  \n",
    "Memory bandwidth: 288 GB/s\n",
    "\n",
    "Compute the bandwidth of each kernel: `bytes_in_and_out / duration_of_kernel`\n",
    "Compute the duration weight of each kernel: `duration_of_kernel / sum_durations_of_all_kernels`<br/>\n",
    "Determine the bandwidth of the GPU by computing the weighted average of the bandwidth of each kernel.<br/>\n",
    "Compute the bandwidth efficiency: `bandwidth_GPU / theoretical_bandwidth`<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "THEORETICAL_FLOPS = 4.29 # in TFLOPS\n",
    "THEORETICAL_BANDWIDTH = 288 # in GB/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select configurations\n",
    "c_num_nodes = [1, 8, 16, 32]\n",
    "c_gpus_per_node = [2]\n",
    "c_network_backend = ['ib']\n",
    "c_profile_level = ['nvprof']\n",
    "c_workers = [2, 8]\n",
    "c_neural_network = ['resnet50']\n",
    "c_data_loader = ['dali-gpu', 'dali-cpu-to-gpu']\n",
    "c_batch_size_per_gpu = [32, 64]\n",
    "c_grad_precision = ['fp16', 'fp32']\n",
    "c_compute_precision = ['fp32']\n",
    "\n",
    "def configurations():\n",
    "    \n",
    "    def to_str(l):\n",
    "        return [str(elem) for elem in l]\n",
    "\n",
    "    confs = [\n",
    "            to_str(c_num_nodes),\n",
    "            to_str(c_gpus_per_node),\n",
    "            c_network_backend,\n",
    "            c_profile_level,\n",
    "            to_str(c_workers),\n",
    "            c_neural_network,\n",
    "            c_data_loader,\n",
    "            to_str(c_batch_size_per_gpu),\n",
    "            c_grad_precision,\n",
    "            c_compute_precision\n",
    "            ]\n",
    "    \n",
    "    return itertools.product(*confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_efficiency(df_csv):\n",
    "#     df_forward = df_csv[df_csv['Direction'] == 'fprop'][['Kernel', 'Sil(ns)', 'FLOPs']]\n",
    "    df_forward = df_csv[['Kernel', 'Sil(ns)', 'FLOPs']]\n",
    "    d = df_forward.groupby('Kernel').sum()\n",
    "    duration_kernels = d.sort_values(by='Sil(ns)', ascending=False)\n",
    "    no_compute = duration_kernels[duration_kernels['FLOPs'] == 0].sum()['Sil(ns)']\n",
    "    compute = duration_kernels[duration_kernels['FLOPs'] != 0].sum()['Sil(ns)']\n",
    "    return compute / (no_compute+compute) * 100 # In %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flops_per_sec(df_csv):\n",
    "    total_duration = df_csv['Sil(ns)'].sum() * 1E-9 # in seconds\n",
    "    total_FLOPs = df_csv['FLOPs'].sum()\n",
    "    return total_FLOPs / total_duration * 1E-12 # in teraFLOPs per seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandwidth(df_csv):\n",
    "    df_csv['bandwidth'] = 1E9 * df_csv['Bytes'] / df_csv['Sil(ns)'] # Bandwidth in bytes/sec\n",
    "    total_duration = df_csv['Sil(ns)'].sum()\n",
    "    df_csv['weight'] = df_csv['Sil(ns)'] / total_duration\n",
    "    df_csv['bandwidth_weighted'] = df_csv['bandwidth'] * df_csv['weight']\n",
    "    return df_csv['bandwidth_weighted'].sum() * 1E-9 # Convert to GB/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for conf in configurations():\n",
    "    node, gpu, network, profile, workers, nn, data_loader, batch_size, grad, comp = conf\n",
    "    file = f'data/pcm/pyprof_kernels/run_0_config_{node}_{gpu}_{network}_{profile}_{workers}_{nn}_{data_loader}_{batch_size}_{grad}_{comp}_ret_0_0.gzip'\n",
    "\n",
    "    df = pd.read_parquet(file)\n",
    "    row = [*conf, compute_efficiency(df), flops_per_sec(df), bandwidth(df)]\n",
    "    rows.append(row)\n",
    "#     break\n",
    "\n",
    "efficiency_df = pd.DataFrame(rows, columns=['nodes', 'gpus_per_node', 'network_backend', 'profile_level', 'workers', 'nn', 'data_loader', 'batch_size', 'grad', 'comp', 'compute_efficiency (%)', 'TFLOPs per sec', 'bandwidth(GB/s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>compute_efficiency (%)</th>\n",
       "      <th>TFLOPs per sec</th>\n",
       "      <th>bandwidth(GB/s)</th>\n",
       "      <th>bandwidth_efficiency (%)</th>\n",
       "      <th>FLOPS_efficiency (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th>nodes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>68.791497</td>\n",
       "      <td>0.347732</td>\n",
       "      <td>25.976727</td>\n",
       "      <td>9.019697</td>\n",
       "      <td>8.105632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>69.840852</td>\n",
       "      <td>0.353306</td>\n",
       "      <td>26.044084</td>\n",
       "      <td>9.043085</td>\n",
       "      <td>8.235571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>69.807003</td>\n",
       "      <td>0.353268</td>\n",
       "      <td>26.052055</td>\n",
       "      <td>9.045853</td>\n",
       "      <td>8.234676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>69.936754</td>\n",
       "      <td>0.354724</td>\n",
       "      <td>26.109891</td>\n",
       "      <td>9.065934</td>\n",
       "      <td>8.268630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">64</th>\n",
       "      <th>1</th>\n",
       "      <td>59.959489</td>\n",
       "      <td>0.386201</td>\n",
       "      <td>28.071793</td>\n",
       "      <td>9.747150</td>\n",
       "      <td>9.002347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>61.145733</td>\n",
       "      <td>0.392053</td>\n",
       "      <td>28.096251</td>\n",
       "      <td>9.755643</td>\n",
       "      <td>9.138773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>61.107659</td>\n",
       "      <td>0.391818</td>\n",
       "      <td>28.079357</td>\n",
       "      <td>9.749777</td>\n",
       "      <td>9.133285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61.224527</td>\n",
       "      <td>0.392876</td>\n",
       "      <td>28.143853</td>\n",
       "      <td>9.772171</td>\n",
       "      <td>9.157942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  compute_efficiency (%)  TFLOPs per sec  bandwidth(GB/s)  \\\n",
       "batch_size nodes                                                            \n",
       "32         1                   68.791497        0.347732        25.976727   \n",
       "           16                  69.840852        0.353306        26.044084   \n",
       "           32                  69.807003        0.353268        26.052055   \n",
       "           8                   69.936754        0.354724        26.109891   \n",
       "64         1                   59.959489        0.386201        28.071793   \n",
       "           16                  61.145733        0.392053        28.096251   \n",
       "           32                  61.107659        0.391818        28.079357   \n",
       "           8                   61.224527        0.392876        28.143853   \n",
       "\n",
       "                  bandwidth_efficiency (%)  FLOPS_efficiency (%)  \n",
       "batch_size nodes                                                  \n",
       "32         1                      9.019697              8.105632  \n",
       "           16                     9.043085              8.235571  \n",
       "           32                     9.045853              8.234676  \n",
       "           8                      9.065934              8.268630  \n",
       "64         1                      9.747150              9.002347  \n",
       "           16                     9.755643              9.138773  \n",
       "           32                     9.749777              9.133285  \n",
       "           8                      9.772171              9.157942  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficiency_df['bandwidth_efficiency (%)'] = efficiency_df['bandwidth(GB/s)'] / THEORETICAL_BANDWIDTH * 100 # in %\n",
    "efficiency_df['FLOPS_efficiency (%)'] = efficiency_df['TFLOPs per sec'] / THEORETICAL_FLOPS * 100 # in %\n",
    "group_df = efficiency_df.groupby(['batch_size', 'nodes']).mean()\n",
    "group_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel-level analysis\n",
    "\n",
    "The maximum efficiency for a single kernel is 33.58 %. However, most of the kernels do not have FLOPs, and therefore a 0 % efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f'data/pcm/pyprof_kernels/run_0_config_8_2_ib_nvprof_2_resnet50_dali-gpu_32_fp16_fp32_ret_0_0.gzip'\n",
    "df = pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel-level analysis\n",
    "# df_forward = df[df['Direction'] == 'fprop'][['Kernel', 'Sil(ns)', 'FLOPs']]\n",
    "df_forward = df[['Kernel', 'Sil(ns)', 'FLOPs']]\n",
    "df_forward_sum = df_forward.groupby('Kernel').sum()\n",
    "duration_kernels = df_forward_sum.sort_values(by='Sil(ns)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sil(ns)</th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs per sec</th>\n",
       "      <th>FLOPS efficiency (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kernel</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cudnn_convolve_sgemm_sm35_ldg_nn_64x16x64x16x16</th>\n",
       "      <td>529095040</td>\n",
       "      <td>7.901765e+11</td>\n",
       "      <td>1.493449e+12</td>\n",
       "      <td>34.812331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::implicit_convolve_sgemm</th>\n",
       "      <td>5755177265</td>\n",
       "      <td>8.142019e+12</td>\n",
       "      <td>1.414729e+12</td>\n",
       "      <td>32.977376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn_convolve_sgemm_sm35_ldg_nn_32x16x64x8x16</th>\n",
       "      <td>5097958</td>\n",
       "      <td>6.576669e+09</td>\n",
       "      <td>1.290059e+12</td>\n",
       "      <td>30.071315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::dgrad_engine</th>\n",
       "      <td>3396871034</td>\n",
       "      <td>2.157969e+12</td>\n",
       "      <td>6.352815e+11</td>\n",
       "      <td>14.808427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::wgrad_alg0_engine</th>\n",
       "      <td>5908700802</td>\n",
       "      <td>3.559365e+12</td>\n",
       "      <td>6.023938e+11</td>\n",
       "      <td>14.041815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::winograd::winograd3x3Kernel</th>\n",
       "      <td>52062179</td>\n",
       "      <td>2.959501e+10</td>\n",
       "      <td>5.684551e+11</td>\n",
       "      <td>13.250701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgemm_sm_heavy_nt_ldg</th>\n",
       "      <td>9172068</td>\n",
       "      <td>3.276800e+09</td>\n",
       "      <td>3.572586e+11</td>\n",
       "      <td>8.327706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fermiPlusCgemmLDS128_batched</th>\n",
       "      <td>226900852</td>\n",
       "      <td>5.507960e+10</td>\n",
       "      <td>2.427474e+11</td>\n",
       "      <td>5.658448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::bn_fw_tr_1C11_kernel_NCHW</th>\n",
       "      <td>1041324312</td>\n",
       "      <td>1.113666e+11</td>\n",
       "      <td>1.069471e+11</td>\n",
       "      <td>2.492940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::bn_fw_tr_1C11_singleread</th>\n",
       "      <td>355409374</td>\n",
       "      <td>3.373754e+10</td>\n",
       "      <td>9.492586e+10</td>\n",
       "      <td>2.212724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::bn_bw_1C11_kernel_new</th>\n",
       "      <td>2280961414</td>\n",
       "      <td>6.775767e+10</td>\n",
       "      <td>2.970575e+10</td>\n",
       "      <td>0.692442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::bn_bw_1C11_singleread</th>\n",
       "      <td>138836884</td>\n",
       "      <td>3.371827e+09</td>\n",
       "      <td>2.428625e+10</td>\n",
       "      <td>0.566113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elementwise_kernel</th>\n",
       "      <td>3815560007</td>\n",
       "      <td>4.528455e+10</td>\n",
       "      <td>1.186839e+10</td>\n",
       "      <td>0.276652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgemm_largek_lds64</th>\n",
       "      <td>3326669983</td>\n",
       "      <td>6.684672e+09</td>\n",
       "      <td>2.009418e+09</td>\n",
       "      <td>0.046840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgemm_sm35_ldg_nn_64x16x64x16x16</th>\n",
       "      <td>2556215858</td>\n",
       "      <td>3.276800e+09</td>\n",
       "      <td>1.281895e+09</td>\n",
       "      <td>0.029881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernelPointwiseApply2</th>\n",
       "      <td>160789126</td>\n",
       "      <td>1.632000e+06</td>\n",
       "      <td>1.014994e+07</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dali::kernels::detail::SliceFlipNormalizePermutePadKernel</th>\n",
       "      <td>34799616</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reduce_kernel</th>\n",
       "      <td>16464632</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernelPointwiseApply1</th>\n",
       "      <td>120323</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flip_filter</th>\n",
       "      <td>41210063</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fft1d_c2r_32</th>\n",
       "      <td>18976523</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nvjpeg::ycbcr_to_format_kernel_roi</th>\n",
       "      <td>16858269</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::dgrad2d_alg1_1</th>\n",
       "      <td>45452050</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nvjpeg::dctQuantInvJpegKernel</th>\n",
       "      <td>45723958</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fft2d_r2c_64x64</th>\n",
       "      <td>50467352</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fft2d_c2r_64x64</th>\n",
       "      <td>27082249</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgemm_sm35_ldg_nt_128x16x64x16x16</th>\n",
       "      <td>2171538</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nvjpeg::dcAcDecodeKernel</th>\n",
       "      <td>1587340</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nvjpeg::transposeKernel</th>\n",
       "      <td>1564941</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::winograd_nonfused::winogradWgradOutput4x4</th>\n",
       "      <td>53628437</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cunn_SoftMaxBackward</th>\n",
       "      <td>472036</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::winograd::generateWinogradTilesKernel</th>\n",
       "      <td>450213</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cunn_ClassNLLCriterion_updateOutput_kernel</th>\n",
       "      <td>421540</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nvjpeg::destuffKernel</th>\n",
       "      <td>397282</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cunn_ClassNLLCriterion_updateGradInput_kernel</th>\n",
       "      <td>247009</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nvjpeg::dcPrefixSumDownWriteKernel</th>\n",
       "      <td>153184</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nvjpeg::dcPrefixSumReadUpKernel</th>\n",
       "      <td>141793</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nvjpeg::dcPrefixSumUpUpDownDownKernel</th>\n",
       "      <td>133026</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cunn_SoftMaxForward</th>\n",
       "      <td>769540</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::winograd_nonfused::winogradForwardFilter4x4</th>\n",
       "      <td>99172855</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fft2d_c2r_16x16</th>\n",
       "      <td>55651198</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>im2col4d_kernel</th>\n",
       "      <td>58373044</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgemm_sm35_ldg_nt_64x16x64x16x16</th>\n",
       "      <td>4184363870</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::dgrad_alg1_engine</th>\n",
       "      <td>1968100726</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::detail::wgrad_alg1_engine</th>\n",
       "      <td>1170887161</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cgemm_strided_batched_sm35_ldg_nt_64x8x64x16x16</th>\n",
       "      <td>503033745</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::winograd_nonfused::winogradForwardOutput4x4</th>\n",
       "      <td>469848694</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn_dgrad_sm35_ldg_nt_64x16x64x16x16</th>\n",
       "      <td>406073037</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::winograd_nonfused::winogradForwardData4x4</th>\n",
       "      <td>387160478</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn_dgrad_sm35_ldg_nt_32x16x64x8x16</th>\n",
       "      <td>367638984</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scalePackedTensor_kernel</th>\n",
       "      <td>348722233</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgemm_sm35_ldg_nt_128x8x128x16x16</th>\n",
       "      <td>347177882</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn_dgrad_sm35_ldg_nt_64x16x128x8x32</th>\n",
       "      <td>318986717</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxPoolBackward</th>\n",
       "      <td>284708326</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::winograd_nonfused::winogradWgradData4x4</th>\n",
       "      <td>232973819</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fft2d_r2c_32x32</th>\n",
       "      <td>217357807</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scal_kernel</th>\n",
       "      <td>168327008</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cgemm_sm35_ldg_tn_64x8x64x16x16</th>\n",
       "      <td>168133129</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudnn::winograd_nonfused::winogradWgradDelta4x4</th>\n",
       "      <td>167282940</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pointwise_mult_and_sum_complex</th>\n",
       "      <td>143114631</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fft2d_r2c_16x16</th>\n",
       "      <td>111559196</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxPoolForward</th>\n",
       "      <td>99223648</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dali::kernels::resampling::BatchedSeparableResampleKernel</th>\n",
       "      <td>61889397</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fft1d_r2c_32</th>\n",
       "      <td>59989952</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fft2d_c2r_32x32</th>\n",
       "      <td>59457438</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compute_gemm_pointers</th>\n",
       "      <td>77888</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Sil(ns)         FLOPs  \\\n",
       "Kernel                                                                         \n",
       "cudnn_convolve_sgemm_sm35_ldg_nn_64x16x64x16x16      529095040  7.901765e+11   \n",
       "cudnn::detail::implicit_convolve_sgemm              5755177265  8.142019e+12   \n",
       "cudnn_convolve_sgemm_sm35_ldg_nn_32x16x64x8x16         5097958  6.576669e+09   \n",
       "cudnn::detail::dgrad_engine                         3396871034  2.157969e+12   \n",
       "cudnn::detail::wgrad_alg0_engine                    5908700802  3.559365e+12   \n",
       "cudnn::winograd::winograd3x3Kernel                    52062179  2.959501e+10   \n",
       "sgemm_sm_heavy_nt_ldg                                  9172068  3.276800e+09   \n",
       "fermiPlusCgemmLDS128_batched                         226900852  5.507960e+10   \n",
       "cudnn::detail::bn_fw_tr_1C11_kernel_NCHW            1041324312  1.113666e+11   \n",
       "cudnn::detail::bn_fw_tr_1C11_singleread              355409374  3.373754e+10   \n",
       "cudnn::detail::bn_bw_1C11_kernel_new                2280961414  6.775767e+10   \n",
       "cudnn::detail::bn_bw_1C11_singleread                 138836884  3.371827e+09   \n",
       "elementwise_kernel                                  3815560007  4.528455e+10   \n",
       "sgemm_largek_lds64                                  3326669983  6.684672e+09   \n",
       "sgemm_sm35_ldg_nn_64x16x64x16x16                    2556215858  3.276800e+09   \n",
       "kernelPointwiseApply2                                160789126  1.632000e+06   \n",
       "dali::kernels::detail::SliceFlipNormalizePermut...    34799616  0.000000e+00   \n",
       "reduce_kernel                                         16464632  0.000000e+00   \n",
       "kernelPointwiseApply1                                   120323  0.000000e+00   \n",
       "flip_filter                                           41210063  0.000000e+00   \n",
       "fft1d_c2r_32                                          18976523  0.000000e+00   \n",
       "nvjpeg::ycbcr_to_format_kernel_roi                    16858269  0.000000e+00   \n",
       "cudnn::detail::dgrad2d_alg1_1                         45452050  0.000000e+00   \n",
       "nvjpeg::dctQuantInvJpegKernel                         45723958  0.000000e+00   \n",
       "fft2d_r2c_64x64                                       50467352  0.000000e+00   \n",
       "fft2d_c2r_64x64                                       27082249  0.000000e+00   \n",
       "sgemm_sm35_ldg_nt_128x16x64x16x16                      2171538  0.000000e+00   \n",
       "nvjpeg::dcAcDecodeKernel                               1587340  0.000000e+00   \n",
       "nvjpeg::transposeKernel                                1564941  0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradWgradOutput4x4      53628437  0.000000e+00   \n",
       "cunn_SoftMaxBackward                                    472036  0.000000e+00   \n",
       "cudnn::winograd::generateWinogradTilesKernel            450213  0.000000e+00   \n",
       "cunn_ClassNLLCriterion_updateOutput_kernel              421540  0.000000e+00   \n",
       "nvjpeg::destuffKernel                                   397282  0.000000e+00   \n",
       "cunn_ClassNLLCriterion_updateGradInput_kernel           247009  0.000000e+00   \n",
       "nvjpeg::dcPrefixSumDownWriteKernel                      153184  0.000000e+00   \n",
       "nvjpeg::dcPrefixSumReadUpKernel                         141793  0.000000e+00   \n",
       "nvjpeg::dcPrefixSumUpUpDownDownKernel                   133026  0.000000e+00   \n",
       "cunn_SoftMaxForward                                     769540  0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradForwardFilter4x4    99172855  0.000000e+00   \n",
       "fft2d_c2r_16x16                                       55651198  0.000000e+00   \n",
       "im2col4d_kernel                                       58373044  0.000000e+00   \n",
       "sgemm_sm35_ldg_nt_64x16x64x16x16                    4184363870  0.000000e+00   \n",
       "cudnn::detail::dgrad_alg1_engine                    1968100726  0.000000e+00   \n",
       "cudnn::detail::wgrad_alg1_engine                    1170887161  0.000000e+00   \n",
       "cgemm_strided_batched_sm35_ldg_nt_64x8x64x16x16      503033745  0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradForwardOutput4x4   469848694  0.000000e+00   \n",
       "cudnn_dgrad_sm35_ldg_nt_64x16x64x16x16               406073037  0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradForwardData4x4     387160478  0.000000e+00   \n",
       "cudnn_dgrad_sm35_ldg_nt_32x16x64x8x16                367638984  0.000000e+00   \n",
       "scalePackedTensor_kernel                             348722233  0.000000e+00   \n",
       "sgemm_sm35_ldg_nt_128x8x128x16x16                    347177882  0.000000e+00   \n",
       "cudnn_dgrad_sm35_ldg_nt_64x16x128x8x32               318986717  0.000000e+00   \n",
       "MaxPoolBackward                                      284708326  0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradWgradData4x4       232973819  0.000000e+00   \n",
       "fft2d_r2c_32x32                                      217357807  0.000000e+00   \n",
       "scal_kernel                                          168327008  0.000000e+00   \n",
       "cgemm_sm35_ldg_tn_64x8x64x16x16                      168133129  0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradWgradDelta4x4      167282940  0.000000e+00   \n",
       "pointwise_mult_and_sum_complex                       143114631  0.000000e+00   \n",
       "fft2d_r2c_16x16                                      111559196  0.000000e+00   \n",
       "MaxPoolForward                                        99223648  0.000000e+00   \n",
       "dali::kernels::resampling::BatchedSeparableResa...    61889397  0.000000e+00   \n",
       "fft1d_r2c_32                                          59989952  0.000000e+00   \n",
       "fft2d_c2r_32x32                                       59457438  0.000000e+00   \n",
       "compute_gemm_pointers                                    77888  0.000000e+00   \n",
       "\n",
       "                                                    FLOPs per sec  \\\n",
       "Kernel                                                              \n",
       "cudnn_convolve_sgemm_sm35_ldg_nn_64x16x64x16x16      1.493449e+12   \n",
       "cudnn::detail::implicit_convolve_sgemm               1.414729e+12   \n",
       "cudnn_convolve_sgemm_sm35_ldg_nn_32x16x64x8x16       1.290059e+12   \n",
       "cudnn::detail::dgrad_engine                          6.352815e+11   \n",
       "cudnn::detail::wgrad_alg0_engine                     6.023938e+11   \n",
       "cudnn::winograd::winograd3x3Kernel                   5.684551e+11   \n",
       "sgemm_sm_heavy_nt_ldg                                3.572586e+11   \n",
       "fermiPlusCgemmLDS128_batched                         2.427474e+11   \n",
       "cudnn::detail::bn_fw_tr_1C11_kernel_NCHW             1.069471e+11   \n",
       "cudnn::detail::bn_fw_tr_1C11_singleread              9.492586e+10   \n",
       "cudnn::detail::bn_bw_1C11_kernel_new                 2.970575e+10   \n",
       "cudnn::detail::bn_bw_1C11_singleread                 2.428625e+10   \n",
       "elementwise_kernel                                   1.186839e+10   \n",
       "sgemm_largek_lds64                                   2.009418e+09   \n",
       "sgemm_sm35_ldg_nn_64x16x64x16x16                     1.281895e+09   \n",
       "kernelPointwiseApply2                                1.014994e+07   \n",
       "dali::kernels::detail::SliceFlipNormalizePermut...   0.000000e+00   \n",
       "reduce_kernel                                        0.000000e+00   \n",
       "kernelPointwiseApply1                                0.000000e+00   \n",
       "flip_filter                                          0.000000e+00   \n",
       "fft1d_c2r_32                                         0.000000e+00   \n",
       "nvjpeg::ycbcr_to_format_kernel_roi                   0.000000e+00   \n",
       "cudnn::detail::dgrad2d_alg1_1                        0.000000e+00   \n",
       "nvjpeg::dctQuantInvJpegKernel                        0.000000e+00   \n",
       "fft2d_r2c_64x64                                      0.000000e+00   \n",
       "fft2d_c2r_64x64                                      0.000000e+00   \n",
       "sgemm_sm35_ldg_nt_128x16x64x16x16                    0.000000e+00   \n",
       "nvjpeg::dcAcDecodeKernel                             0.000000e+00   \n",
       "nvjpeg::transposeKernel                              0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradWgradOutput4x4     0.000000e+00   \n",
       "cunn_SoftMaxBackward                                 0.000000e+00   \n",
       "cudnn::winograd::generateWinogradTilesKernel         0.000000e+00   \n",
       "cunn_ClassNLLCriterion_updateOutput_kernel           0.000000e+00   \n",
       "nvjpeg::destuffKernel                                0.000000e+00   \n",
       "cunn_ClassNLLCriterion_updateGradInput_kernel        0.000000e+00   \n",
       "nvjpeg::dcPrefixSumDownWriteKernel                   0.000000e+00   \n",
       "nvjpeg::dcPrefixSumReadUpKernel                      0.000000e+00   \n",
       "nvjpeg::dcPrefixSumUpUpDownDownKernel                0.000000e+00   \n",
       "cunn_SoftMaxForward                                  0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradForwardFilter4x4   0.000000e+00   \n",
       "fft2d_c2r_16x16                                      0.000000e+00   \n",
       "im2col4d_kernel                                      0.000000e+00   \n",
       "sgemm_sm35_ldg_nt_64x16x64x16x16                     0.000000e+00   \n",
       "cudnn::detail::dgrad_alg1_engine                     0.000000e+00   \n",
       "cudnn::detail::wgrad_alg1_engine                     0.000000e+00   \n",
       "cgemm_strided_batched_sm35_ldg_nt_64x8x64x16x16      0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradForwardOutput4x4   0.000000e+00   \n",
       "cudnn_dgrad_sm35_ldg_nt_64x16x64x16x16               0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradForwardData4x4     0.000000e+00   \n",
       "cudnn_dgrad_sm35_ldg_nt_32x16x64x8x16                0.000000e+00   \n",
       "scalePackedTensor_kernel                             0.000000e+00   \n",
       "sgemm_sm35_ldg_nt_128x8x128x16x16                    0.000000e+00   \n",
       "cudnn_dgrad_sm35_ldg_nt_64x16x128x8x32               0.000000e+00   \n",
       "MaxPoolBackward                                      0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradWgradData4x4       0.000000e+00   \n",
       "fft2d_r2c_32x32                                      0.000000e+00   \n",
       "scal_kernel                                          0.000000e+00   \n",
       "cgemm_sm35_ldg_tn_64x8x64x16x16                      0.000000e+00   \n",
       "cudnn::winograd_nonfused::winogradWgradDelta4x4      0.000000e+00   \n",
       "pointwise_mult_and_sum_complex                       0.000000e+00   \n",
       "fft2d_r2c_16x16                                      0.000000e+00   \n",
       "MaxPoolForward                                       0.000000e+00   \n",
       "dali::kernels::resampling::BatchedSeparableResa...   0.000000e+00   \n",
       "fft1d_r2c_32                                         0.000000e+00   \n",
       "fft2d_c2r_32x32                                      0.000000e+00   \n",
       "compute_gemm_pointers                                0.000000e+00   \n",
       "\n",
       "                                                    FLOPS efficiency (%)  \n",
       "Kernel                                                                    \n",
       "cudnn_convolve_sgemm_sm35_ldg_nn_64x16x64x16x16                34.812331  \n",
       "cudnn::detail::implicit_convolve_sgemm                         32.977376  \n",
       "cudnn_convolve_sgemm_sm35_ldg_nn_32x16x64x8x16                 30.071315  \n",
       "cudnn::detail::dgrad_engine                                    14.808427  \n",
       "cudnn::detail::wgrad_alg0_engine                               14.041815  \n",
       "cudnn::winograd::winograd3x3Kernel                             13.250701  \n",
       "sgemm_sm_heavy_nt_ldg                                           8.327706  \n",
       "fermiPlusCgemmLDS128_batched                                    5.658448  \n",
       "cudnn::detail::bn_fw_tr_1C11_kernel_NCHW                        2.492940  \n",
       "cudnn::detail::bn_fw_tr_1C11_singleread                         2.212724  \n",
       "cudnn::detail::bn_bw_1C11_kernel_new                            0.692442  \n",
       "cudnn::detail::bn_bw_1C11_singleread                            0.566113  \n",
       "elementwise_kernel                                              0.276652  \n",
       "sgemm_largek_lds64                                              0.046840  \n",
       "sgemm_sm35_ldg_nn_64x16x64x16x16                                0.029881  \n",
       "kernelPointwiseApply2                                           0.000237  \n",
       "dali::kernels::detail::SliceFlipNormalizePermut...              0.000000  \n",
       "reduce_kernel                                                   0.000000  \n",
       "kernelPointwiseApply1                                           0.000000  \n",
       "flip_filter                                                     0.000000  \n",
       "fft1d_c2r_32                                                    0.000000  \n",
       "nvjpeg::ycbcr_to_format_kernel_roi                              0.000000  \n",
       "cudnn::detail::dgrad2d_alg1_1                                   0.000000  \n",
       "nvjpeg::dctQuantInvJpegKernel                                   0.000000  \n",
       "fft2d_r2c_64x64                                                 0.000000  \n",
       "fft2d_c2r_64x64                                                 0.000000  \n",
       "sgemm_sm35_ldg_nt_128x16x64x16x16                               0.000000  \n",
       "nvjpeg::dcAcDecodeKernel                                        0.000000  \n",
       "nvjpeg::transposeKernel                                         0.000000  \n",
       "cudnn::winograd_nonfused::winogradWgradOutput4x4                0.000000  \n",
       "cunn_SoftMaxBackward                                            0.000000  \n",
       "cudnn::winograd::generateWinogradTilesKernel                    0.000000  \n",
       "cunn_ClassNLLCriterion_updateOutput_kernel                      0.000000  \n",
       "nvjpeg::destuffKernel                                           0.000000  \n",
       "cunn_ClassNLLCriterion_updateGradInput_kernel                   0.000000  \n",
       "nvjpeg::dcPrefixSumDownWriteKernel                              0.000000  \n",
       "nvjpeg::dcPrefixSumReadUpKernel                                 0.000000  \n",
       "nvjpeg::dcPrefixSumUpUpDownDownKernel                           0.000000  \n",
       "cunn_SoftMaxForward                                             0.000000  \n",
       "cudnn::winograd_nonfused::winogradForwardFilter4x4              0.000000  \n",
       "fft2d_c2r_16x16                                                 0.000000  \n",
       "im2col4d_kernel                                                 0.000000  \n",
       "sgemm_sm35_ldg_nt_64x16x64x16x16                                0.000000  \n",
       "cudnn::detail::dgrad_alg1_engine                                0.000000  \n",
       "cudnn::detail::wgrad_alg1_engine                                0.000000  \n",
       "cgemm_strided_batched_sm35_ldg_nt_64x8x64x16x16                 0.000000  \n",
       "cudnn::winograd_nonfused::winogradForwardOutput4x4              0.000000  \n",
       "cudnn_dgrad_sm35_ldg_nt_64x16x64x16x16                          0.000000  \n",
       "cudnn::winograd_nonfused::winogradForwardData4x4                0.000000  \n",
       "cudnn_dgrad_sm35_ldg_nt_32x16x64x8x16                           0.000000  \n",
       "scalePackedTensor_kernel                                        0.000000  \n",
       "sgemm_sm35_ldg_nt_128x8x128x16x16                               0.000000  \n",
       "cudnn_dgrad_sm35_ldg_nt_64x16x128x8x32                          0.000000  \n",
       "MaxPoolBackward                                                 0.000000  \n",
       "cudnn::winograd_nonfused::winogradWgradData4x4                  0.000000  \n",
       "fft2d_r2c_32x32                                                 0.000000  \n",
       "scal_kernel                                                     0.000000  \n",
       "cgemm_sm35_ldg_tn_64x8x64x16x16                                 0.000000  \n",
       "cudnn::winograd_nonfused::winogradWgradDelta4x4                 0.000000  \n",
       "pointwise_mult_and_sum_complex                                  0.000000  \n",
       "fft2d_r2c_16x16                                                 0.000000  \n",
       "MaxPoolForward                                                  0.000000  \n",
       "dali::kernels::resampling::BatchedSeparableResa...              0.000000  \n",
       "fft1d_r2c_32                                                    0.000000  \n",
       "fft2d_c2r_32x32                                                 0.000000  \n",
       "compute_gemm_pointers                                           0.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_kernels['FLOPs per sec'] = duration_kernels['FLOPs'] / duration_kernels['Sil(ns)'] * 1E9\n",
    "duration_kernels['FLOPS efficiency (%)'] = duration_kernels['FLOPs per sec'] * 1E-12 / THEORETICAL_FLOPS * 100\n",
    "final = duration_kernels.sort_values(by=['Sil(ns)', 'FLOPS efficiency (%)'], ascending=False)\n",
    "final.sort_values(by='FLOPS efficiency (%)', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled FLOPS and bandwidth efficiencies\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Sum the flops/transferred bytes of all kernels and divide by the duration of all the batches. The duration of a single batch is computed as the minimum of the tau batch duration, nvprof batch duration, and pyprof batch duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tau batch duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init_times = pd.read_parquet('data/mpi/batch1_5runs_interval.gzip')\n",
    "\n",
    "def get_init_time(conf):\n",
    "    nodes, gpus, network, profile, workers, nn, data_loader, batch_size, grad, comp = conf\n",
    "    \n",
    "    init = df_init_times[\n",
    "        (df_init_times['nodes'] == int(nodes)) & \n",
    "        (df_init_times['workers'] == int(workers)) & \n",
    "        (df_init_times['data_loader'] == data_loader) & \n",
    "        (df_init_times['batch_size_per_gpu'] == int(batch_size)) & \n",
    "        (df_init_times['grad_precision'] == grad) &\n",
    "        (df_init_times['gpu'] == 0) &\n",
    "        (df_init_times['thread'] == 0) &\n",
    "        (df_init_times['function'] == '.tau application')\n",
    "    ].inc_time.values\n",
    "    init_time = np.median(init) * 1E-6 # in seconds\n",
    "    \n",
    "    return init_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_file = f'data/mpi/interval.gzip'\n",
    "df_tau = pd.read_parquet(tau_file)\n",
    "NUM_BATCHES = 50\n",
    "\n",
    "def tau_batch_duration(conf):\n",
    "    node, gpu, network, profile, workers, nn, data_loader, batch_size, grad, comp = conf\n",
    "    \n",
    "    df_conf = df_tau[\n",
    "        (df_tau['nodes']== int(node)) & \n",
    "        (df_tau['workers'] == int(workers)) & \n",
    "        (df_tau['data_loader'] == data_loader) & \n",
    "        (df_tau['batch_size_per_gpu'] == int(batch_size)) &\n",
    "        (df_tau['grad_precision'] == grad) &\n",
    "        (df_tau['gpu'] == 0) &\n",
    "        (df_tau['thread'] == 0) &\n",
    "        (df_tau['function'] == '.tau application')\n",
    "    ]\n",
    "    \n",
    "    exp_duration = np.max(df_conf.inc_time.values) * 1E-6 - get_init_time(conf)\n",
    "    batch_duration_tau = exp_duration / NUM_BATCHES\n",
    "    \n",
    "    return batch_duration_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvprof batch duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvprof_batch_duration(conf):    \n",
    "    node, gpu, network, profile, workers, nn, data_loader, batch_size, grad, comp = conf\n",
    "    \n",
    "    nvprof_marker_file = f'data/pcm/nvprof_markers/markers_run_0_config_{node}_{gpu}_{network}_{profile}_{workers}_{nn}_{data_loader}_{batch_size}_{grad}_{comp}_ret_0_0.csv'\n",
    "    df_nvprof = pd.read_csv(nvprof_marker_file)\n",
    "    \n",
    "    end_batch_markers = df_nvprof[df_nvprof['name_str'] == 'End of Batch'].timestamp.values\n",
    "    batch_duration_nvprof = np.median(np.diff(end_batch_markers) * 1E-9)\n",
    "    \n",
    "    return batch_duration_nvprof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyprof batch duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyprof_batch_duration(conf):\n",
    "    node, gpu, network, profile, workers, nn, data_loader, batch_size, grad, comp = conf\n",
    "    \n",
    "    nvprof_marker_file = f'data/pcm/nvprof_markers/markers_run_0_config_{node}_{gpu}_{network}_{profile}_{workers}_{nn}_{data_loader}_{batch_size}_{grad}_{comp}_ret_0_0.csv'\n",
    "    df_nvprof = pd.read_csv(nvprof_marker_file)\n",
    "    \n",
    "    forward_markers = df_nvprof[df_nvprof['name_str'] == 'Forward Pass'].timestamp.values # pyprof batches do not have 'End of Batch' markers\n",
    "    batch_duration_pyprof = np.median(np.diff(forward_markers[26:]) * 1E-9) # Only last 25 batches have pyprof enabled\n",
    "    \n",
    "    return batch_duration_pyprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_duration(conf):\n",
    "    return min(tau_batch_duration(conf), nvprof_batch_duration(conf), pyprof_batch_duration(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled efficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for conf in configurations():\n",
    "#     print(conf)\n",
    "    node, gpu, network, profile, workers, nn, data_loader, batch_size, grad, comp = conf\n",
    "    file = f'data/pcm/pyprof_kernels/run_0_config_{node}_{gpu}_{network}_{profile}_{workers}_{nn}_{data_loader}_{batch_size}_{grad}_{comp}_ret_0_0.gzip'\n",
    "    df = pd.read_parquet(file)\n",
    "    \n",
    "    batch = batch_duration(conf)\n",
    "    \n",
    "    bandwidth_efficiency = df['Bytes'].sum() / (25 * batch) * 1E-9 / THEORETICAL_BANDWIDTH * 100\n",
    "    flop_efficiency = df['FLOPs'].sum() / (25 * batch) * 1E-12 / THEORETICAL_FLOPS * 100\n",
    "    \n",
    "    data.append((*conf, flop_efficiency, bandwidth_efficiency))\n",
    "\n",
    "df_efficiencies = pd.DataFrame(data, columns=['nodes', 'gpu_per_nodes', 'network_backend', 'profile', 'workers', 'nn', 'data_loader', 'batch_size', 'grad', 'comp', 'bandwidth_efficiency (%)', 'FLOPS_efficiency (%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>gpu_per_nodes</th>\n",
       "      <th>network_backend</th>\n",
       "      <th>profile</th>\n",
       "      <th>workers</th>\n",
       "      <th>nn</th>\n",
       "      <th>data_loader</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>grad</th>\n",
       "      <th>comp</th>\n",
       "      <th>bandwidth_efficiency (%)</th>\n",
       "      <th>FLOPS_efficiency (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.714534</td>\n",
       "      <td>17.460536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.082657</td>\n",
       "      <td>16.758452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.254345</td>\n",
       "      <td>19.674580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.532566</td>\n",
       "      <td>18.896645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>12.927701</td>\n",
       "      <td>14.364065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>13.101616</td>\n",
       "      <td>14.557303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.562210</td>\n",
       "      <td>16.772991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>14.208194</td>\n",
       "      <td>15.500853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.911288</td>\n",
       "      <td>17.893018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>14.994187</td>\n",
       "      <td>16.660152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.724147</td>\n",
       "      <td>20.180935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.562105</td>\n",
       "      <td>19.159903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>11.442657</td>\n",
       "      <td>12.714022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>11.509083</td>\n",
       "      <td>12.787827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>11.776408</td>\n",
       "      <td>12.692642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>11.830490</td>\n",
       "      <td>12.906827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.499040</td>\n",
       "      <td>20.244694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>16.473386</td>\n",
       "      <td>18.027890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.941271</td>\n",
       "      <td>22.227165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>19.057495</td>\n",
       "      <td>20.791344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.139024</td>\n",
       "      <td>19.850705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>16.233587</td>\n",
       "      <td>17.765462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.763471</td>\n",
       "      <td>22.038449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>19.093546</td>\n",
       "      <td>20.579073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.984753</td>\n",
       "      <td>19.982993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.880997</td>\n",
       "      <td>17.379601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.978386</td>\n",
       "      <td>22.266560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>19.287987</td>\n",
       "      <td>20.472362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.405132</td>\n",
       "      <td>20.141924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>16.562558</td>\n",
       "      <td>18.125476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.834878</td>\n",
       "      <td>22.114240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>19.521553</td>\n",
       "      <td>20.720270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>12.785983</td>\n",
       "      <td>13.992526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.997145</td>\n",
       "      <td>17.714814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.593496</td>\n",
       "      <td>22.195723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>19.101281</td>\n",
       "      <td>20.587410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.782686</td>\n",
       "      <td>19.758475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.915163</td>\n",
       "      <td>17.416991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.658280</td>\n",
       "      <td>21.926798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.565205</td>\n",
       "      <td>19.705198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.787539</td>\n",
       "      <td>19.466053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.586280</td>\n",
       "      <td>17.057072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.766612</td>\n",
       "      <td>22.041782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.780643</td>\n",
       "      <td>20.241826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.235215</td>\n",
       "      <td>19.955973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.347788</td>\n",
       "      <td>16.796075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.733904</td>\n",
       "      <td>22.007065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.989491</td>\n",
       "      <td>19.094133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.121801</td>\n",
       "      <td>19.831857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.878208</td>\n",
       "      <td>17.376548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.737506</td>\n",
       "      <td>22.010889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.877323</td>\n",
       "      <td>20.346027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.380117</td>\n",
       "      <td>19.311177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.613113</td>\n",
       "      <td>17.086437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.438941</td>\n",
       "      <td>21.693991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>19.030082</td>\n",
       "      <td>20.198620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.386653</td>\n",
       "      <td>19.027337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>14.789564</td>\n",
       "      <td>16.432795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.132901</td>\n",
       "      <td>21.699293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.654636</td>\n",
       "      <td>19.800121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>17.953726</td>\n",
       "      <td>19.647922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>15.960220</td>\n",
       "      <td>17.466299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp16</td>\n",
       "      <td>fp32</td>\n",
       "      <td>20.487417</td>\n",
       "      <td>21.745443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>ib</td>\n",
       "      <td>nvprof</td>\n",
       "      <td>8</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>dali-cpu-to-gpu</td>\n",
       "      <td>64</td>\n",
       "      <td>fp32</td>\n",
       "      <td>fp32</td>\n",
       "      <td>18.675951</td>\n",
       "      <td>20.128988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nodes gpu_per_nodes network_backend profile workers        nn  \\\n",
       "0      1             2              ib  nvprof       2  resnet50   \n",
       "1      1             2              ib  nvprof       2  resnet50   \n",
       "2      1             2              ib  nvprof       2  resnet50   \n",
       "3      1             2              ib  nvprof       2  resnet50   \n",
       "4      1             2              ib  nvprof       2  resnet50   \n",
       "5      1             2              ib  nvprof       2  resnet50   \n",
       "6      1             2              ib  nvprof       2  resnet50   \n",
       "7      1             2              ib  nvprof       2  resnet50   \n",
       "8      1             2              ib  nvprof       8  resnet50   \n",
       "9      1             2              ib  nvprof       8  resnet50   \n",
       "10     1             2              ib  nvprof       8  resnet50   \n",
       "11     1             2              ib  nvprof       8  resnet50   \n",
       "12     1             2              ib  nvprof       8  resnet50   \n",
       "13     1             2              ib  nvprof       8  resnet50   \n",
       "14     1             2              ib  nvprof       8  resnet50   \n",
       "15     1             2              ib  nvprof       8  resnet50   \n",
       "16     8             2              ib  nvprof       2  resnet50   \n",
       "17     8             2              ib  nvprof       2  resnet50   \n",
       "18     8             2              ib  nvprof       2  resnet50   \n",
       "19     8             2              ib  nvprof       2  resnet50   \n",
       "20     8             2              ib  nvprof       2  resnet50   \n",
       "21     8             2              ib  nvprof       2  resnet50   \n",
       "22     8             2              ib  nvprof       2  resnet50   \n",
       "23     8             2              ib  nvprof       2  resnet50   \n",
       "24     8             2              ib  nvprof       8  resnet50   \n",
       "25     8             2              ib  nvprof       8  resnet50   \n",
       "26     8             2              ib  nvprof       8  resnet50   \n",
       "27     8             2              ib  nvprof       8  resnet50   \n",
       "28     8             2              ib  nvprof       8  resnet50   \n",
       "29     8             2              ib  nvprof       8  resnet50   \n",
       "30     8             2              ib  nvprof       8  resnet50   \n",
       "31     8             2              ib  nvprof       8  resnet50   \n",
       "32    16             2              ib  nvprof       2  resnet50   \n",
       "33    16             2              ib  nvprof       2  resnet50   \n",
       "34    16             2              ib  nvprof       2  resnet50   \n",
       "35    16             2              ib  nvprof       2  resnet50   \n",
       "36    16             2              ib  nvprof       2  resnet50   \n",
       "37    16             2              ib  nvprof       2  resnet50   \n",
       "38    16             2              ib  nvprof       2  resnet50   \n",
       "39    16             2              ib  nvprof       2  resnet50   \n",
       "40    16             2              ib  nvprof       8  resnet50   \n",
       "41    16             2              ib  nvprof       8  resnet50   \n",
       "42    16             2              ib  nvprof       8  resnet50   \n",
       "43    16             2              ib  nvprof       8  resnet50   \n",
       "44    16             2              ib  nvprof       8  resnet50   \n",
       "45    16             2              ib  nvprof       8  resnet50   \n",
       "46    16             2              ib  nvprof       8  resnet50   \n",
       "47    16             2              ib  nvprof       8  resnet50   \n",
       "48    32             2              ib  nvprof       2  resnet50   \n",
       "49    32             2              ib  nvprof       2  resnet50   \n",
       "50    32             2              ib  nvprof       2  resnet50   \n",
       "51    32             2              ib  nvprof       2  resnet50   \n",
       "52    32             2              ib  nvprof       2  resnet50   \n",
       "53    32             2              ib  nvprof       2  resnet50   \n",
       "54    32             2              ib  nvprof       2  resnet50   \n",
       "55    32             2              ib  nvprof       2  resnet50   \n",
       "56    32             2              ib  nvprof       8  resnet50   \n",
       "57    32             2              ib  nvprof       8  resnet50   \n",
       "58    32             2              ib  nvprof       8  resnet50   \n",
       "59    32             2              ib  nvprof       8  resnet50   \n",
       "60    32             2              ib  nvprof       8  resnet50   \n",
       "61    32             2              ib  nvprof       8  resnet50   \n",
       "62    32             2              ib  nvprof       8  resnet50   \n",
       "63    32             2              ib  nvprof       8  resnet50   \n",
       "\n",
       "        data_loader batch_size  grad  comp  bandwidth_efficiency (%)  \\\n",
       "0          dali-gpu         32  fp16  fp32                 15.714534   \n",
       "1          dali-gpu         32  fp32  fp32                 15.082657   \n",
       "2          dali-gpu         64  fp16  fp32                 18.254345   \n",
       "3          dali-gpu         64  fp32  fp32                 17.532566   \n",
       "4   dali-cpu-to-gpu         32  fp16  fp32                 12.927701   \n",
       "5   dali-cpu-to-gpu         32  fp32  fp32                 13.101616   \n",
       "6   dali-cpu-to-gpu         64  fp16  fp32                 15.562210   \n",
       "7   dali-cpu-to-gpu         64  fp32  fp32                 14.208194   \n",
       "8          dali-gpu         32  fp16  fp32                 15.911288   \n",
       "9          dali-gpu         32  fp32  fp32                 14.994187   \n",
       "10         dali-gpu         64  fp16  fp32                 18.724147   \n",
       "11         dali-gpu         64  fp32  fp32                 17.562105   \n",
       "12  dali-cpu-to-gpu         32  fp16  fp32                 11.442657   \n",
       "13  dali-cpu-to-gpu         32  fp32  fp32                 11.509083   \n",
       "14  dali-cpu-to-gpu         64  fp16  fp32                 11.776408   \n",
       "15  dali-cpu-to-gpu         64  fp32  fp32                 11.830490   \n",
       "16         dali-gpu         32  fp16  fp32                 18.499040   \n",
       "17         dali-gpu         32  fp32  fp32                 16.473386   \n",
       "18         dali-gpu         64  fp16  fp32                 20.941271   \n",
       "19         dali-gpu         64  fp32  fp32                 19.057495   \n",
       "20  dali-cpu-to-gpu         32  fp16  fp32                 18.139024   \n",
       "21  dali-cpu-to-gpu         32  fp32  fp32                 16.233587   \n",
       "22  dali-cpu-to-gpu         64  fp16  fp32                 20.763471   \n",
       "23  dali-cpu-to-gpu         64  fp32  fp32                 19.093546   \n",
       "24         dali-gpu         32  fp16  fp32                 17.984753   \n",
       "25         dali-gpu         32  fp32  fp32                 15.880997   \n",
       "26         dali-gpu         64  fp16  fp32                 20.978386   \n",
       "27         dali-gpu         64  fp32  fp32                 19.287987   \n",
       "28  dali-cpu-to-gpu         32  fp16  fp32                 18.405132   \n",
       "29  dali-cpu-to-gpu         32  fp32  fp32                 16.562558   \n",
       "30  dali-cpu-to-gpu         64  fp16  fp32                 20.834878   \n",
       "31  dali-cpu-to-gpu         64  fp32  fp32                 19.521553   \n",
       "32         dali-gpu         32  fp16  fp32                 12.785983   \n",
       "33         dali-gpu         32  fp32  fp32                 15.997145   \n",
       "34         dali-gpu         64  fp16  fp32                 20.593496   \n",
       "35         dali-gpu         64  fp32  fp32                 19.101281   \n",
       "36  dali-cpu-to-gpu         32  fp16  fp32                 17.782686   \n",
       "37  dali-cpu-to-gpu         32  fp32  fp32                 15.915163   \n",
       "38  dali-cpu-to-gpu         64  fp16  fp32                 20.658280   \n",
       "39  dali-cpu-to-gpu         64  fp32  fp32                 18.565205   \n",
       "40         dali-gpu         32  fp16  fp32                 17.787539   \n",
       "41         dali-gpu         32  fp32  fp32                 15.586280   \n",
       "42         dali-gpu         64  fp16  fp32                 20.766612   \n",
       "43         dali-gpu         64  fp32  fp32                 18.780643   \n",
       "44  dali-cpu-to-gpu         32  fp16  fp32                 18.235215   \n",
       "45  dali-cpu-to-gpu         32  fp32  fp32                 15.347788   \n",
       "46  dali-cpu-to-gpu         64  fp16  fp32                 20.733904   \n",
       "47  dali-cpu-to-gpu         64  fp32  fp32                 17.989491   \n",
       "48         dali-gpu         32  fp16  fp32                 18.121801   \n",
       "49         dali-gpu         32  fp32  fp32                 15.878208   \n",
       "50         dali-gpu         64  fp16  fp32                 20.737506   \n",
       "51         dali-gpu         64  fp32  fp32                 18.877323   \n",
       "52  dali-cpu-to-gpu         32  fp16  fp32                 17.380117   \n",
       "53  dali-cpu-to-gpu         32  fp32  fp32                 15.613113   \n",
       "54  dali-cpu-to-gpu         64  fp16  fp32                 20.438941   \n",
       "55  dali-cpu-to-gpu         64  fp32  fp32                 19.030082   \n",
       "56         dali-gpu         32  fp16  fp32                 17.386653   \n",
       "57         dali-gpu         32  fp32  fp32                 14.789564   \n",
       "58         dali-gpu         64  fp16  fp32                 20.132901   \n",
       "59         dali-gpu         64  fp32  fp32                 18.654636   \n",
       "60  dali-cpu-to-gpu         32  fp16  fp32                 17.953726   \n",
       "61  dali-cpu-to-gpu         32  fp32  fp32                 15.960220   \n",
       "62  dali-cpu-to-gpu         64  fp16  fp32                 20.487417   \n",
       "63  dali-cpu-to-gpu         64  fp32  fp32                 18.675951   \n",
       "\n",
       "    FLOPS_efficiency (%)  \n",
       "0              17.460536  \n",
       "1              16.758452  \n",
       "2              19.674580  \n",
       "3              18.896645  \n",
       "4              14.364065  \n",
       "5              14.557303  \n",
       "6              16.772991  \n",
       "7              15.500853  \n",
       "8              17.893018  \n",
       "9              16.660152  \n",
       "10             20.180935  \n",
       "11             19.159903  \n",
       "12             12.714022  \n",
       "13             12.787827  \n",
       "14             12.692642  \n",
       "15             12.906827  \n",
       "16             20.244694  \n",
       "17             18.027890  \n",
       "18             22.227165  \n",
       "19             20.791344  \n",
       "20             19.850705  \n",
       "21             17.765462  \n",
       "22             22.038449  \n",
       "23             20.579073  \n",
       "24             19.982993  \n",
       "25             17.379601  \n",
       "26             22.266560  \n",
       "27             20.472362  \n",
       "28             20.141924  \n",
       "29             18.125476  \n",
       "30             22.114240  \n",
       "31             20.720270  \n",
       "32             13.992526  \n",
       "33             17.714814  \n",
       "34             22.195723  \n",
       "35             20.587410  \n",
       "36             19.758475  \n",
       "37             17.416991  \n",
       "38             21.926798  \n",
       "39             19.705198  \n",
       "40             19.466053  \n",
       "41             17.057072  \n",
       "42             22.041782  \n",
       "43             20.241826  \n",
       "44             19.955973  \n",
       "45             16.796075  \n",
       "46             22.007065  \n",
       "47             19.094133  \n",
       "48             19.831857  \n",
       "49             17.376548  \n",
       "50             22.010889  \n",
       "51             20.346027  \n",
       "52             19.311177  \n",
       "53             17.086437  \n",
       "54             21.693991  \n",
       "55             20.198620  \n",
       "56             19.027337  \n",
       "57             16.432795  \n",
       "58             21.699293  \n",
       "59             19.800121  \n",
       "60             19.647922  \n",
       "61             17.466299  \n",
       "62             21.745443  \n",
       "63             20.128988  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_efficiencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bandwidth_efficiency (%)</th>\n",
       "      <th>FLOPS_efficiency (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grad</th>\n",
       "      <th>nodes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">fp16</th>\n",
       "      <th>1</th>\n",
       "      <td>15.039161</td>\n",
       "      <td>16.469099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.667964</td>\n",
       "      <td>20.168049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>19.079883</td>\n",
       "      <td>20.620989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19.568244</td>\n",
       "      <td>21.108341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">fp32</th>\n",
       "      <th>1</th>\n",
       "      <td>14.477612</td>\n",
       "      <td>15.903495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.160375</td>\n",
       "      <td>18.576690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>17.184887</td>\n",
       "      <td>18.604480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.763889</td>\n",
       "      <td>19.232685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bandwidth_efficiency (%)  FLOPS_efficiency (%)\n",
       "grad nodes                                                \n",
       "fp16 1                     15.039161             16.469099\n",
       "     16                    18.667964             20.168049\n",
       "     32                    19.079883             20.620989\n",
       "     8                     19.568244             21.108341\n",
       "fp32 1                     14.477612             15.903495\n",
       "     16                    17.160375             18.576690\n",
       "     32                    17.184887             18.604480\n",
       "     8                     17.763889             19.232685"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_efficiencies.groupby(['grad', 'nodes']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bandwidth_efficiency (%)</th>\n",
       "      <th>FLOPS_efficiency (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_loader</th>\n",
       "      <th>nodes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dali-cpu-to-gpu</th>\n",
       "      <th>1</th>\n",
       "      <td>12.794795</td>\n",
       "      <td>14.037066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.153467</td>\n",
       "      <td>19.582588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>18.192446</td>\n",
       "      <td>19.659860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.694219</td>\n",
       "      <td>20.166950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dali-gpu</th>\n",
       "      <th>1</th>\n",
       "      <td>16.721979</td>\n",
       "      <td>18.335528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.674872</td>\n",
       "      <td>19.162151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>18.072324</td>\n",
       "      <td>19.565608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.637914</td>\n",
       "      <td>20.174076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       bandwidth_efficiency (%)  FLOPS_efficiency (%)\n",
       "data_loader     nodes                                                \n",
       "dali-cpu-to-gpu 1                     12.794795             14.037066\n",
       "                16                    18.153467             19.582588\n",
       "                32                    18.192446             19.659860\n",
       "                8                     18.694219             20.166950\n",
       "dali-gpu        1                     16.721979             18.335528\n",
       "                16                    17.674872             19.162151\n",
       "                32                    18.072324             19.565608\n",
       "                8                     18.637914             20.174076"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_efficiencies.groupby(['data_loader', 'nodes']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bandwidth_efficiency (%)</th>\n",
       "      <th>FLOPS_efficiency (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th>nodes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>13.835465</td>\n",
       "      <td>15.399422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.179725</td>\n",
       "      <td>17.769747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16.635425</td>\n",
       "      <td>18.272546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.272310</td>\n",
       "      <td>18.939843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">64</th>\n",
       "      <th>1</th>\n",
       "      <td>15.681308</td>\n",
       "      <td>16.973172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19.648614</td>\n",
       "      <td>20.974992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>19.629345</td>\n",
       "      <td>20.952922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20.059823</td>\n",
       "      <td>21.401183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bandwidth_efficiency (%)  FLOPS_efficiency (%)\n",
       "batch_size nodes                                                \n",
       "32         1                     13.835465             15.399422\n",
       "           16                    16.179725             17.769747\n",
       "           32                    16.635425             18.272546\n",
       "           8                     17.272310             18.939843\n",
       "64         1                     15.681308             16.973172\n",
       "           16                    19.648614             20.974992\n",
       "           32                    19.629345             20.952922\n",
       "           8                     20.059823             21.401183"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_efficiencies.groupby(['batch_size', 'nodes']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bandwidth_efficiency (%)</th>\n",
       "      <th>FLOPS_efficiency (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workers</th>\n",
       "      <th>nodes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>1</th>\n",
       "      <td>15.297978</td>\n",
       "      <td>16.748178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.674905</td>\n",
       "      <td>19.162242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>18.259636</td>\n",
       "      <td>19.731943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.650103</td>\n",
       "      <td>20.190598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>14.218796</td>\n",
       "      <td>15.624416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.153434</td>\n",
       "      <td>19.582497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>18.005134</td>\n",
       "      <td>19.493525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.682030</td>\n",
       "      <td>20.150428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bandwidth_efficiency (%)  FLOPS_efficiency (%)\n",
       "workers nodes                                                \n",
       "2       1                     15.297978             16.748178\n",
       "        16                    17.674905             19.162242\n",
       "        32                    18.259636             19.731943\n",
       "        8                     18.650103             20.190598\n",
       "8       1                     14.218796             15.624416\n",
       "        16                    18.153434             19.582497\n",
       "        32                    18.005134             19.493525\n",
       "        8                     18.682030             20.150428"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_efficiencies.groupby(['workers', 'nodes']).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
